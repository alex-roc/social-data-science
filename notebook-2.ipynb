{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63dbc05c",
   "metadata": {},
   "source": [
    "# Análisis de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib scikit-learn spacy gdeltdoc pysentimiento unidecode wordcloud\n",
    "%python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1430b7d",
   "metadata": {},
   "source": [
    "## Descarga de titulares con GDELT (últimos 30 días)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69069d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, datetime as dt\n",
    "from gdeltdoc import Filters, GdeltDoc  # Cliente oficial de la comunidad\n",
    "# Referencia: https://github.com/alex9smith/gdelt-doc-api  (instalación/uso)  # :contentReference[oaicite:12]{index=12}\n",
    "\n",
    "gd = GdeltDoc()\n",
    "\n",
    "PAISES = {\n",
    "    \"Bolivia\": [\"Bolivia\", \"La Paz\", \"Cochabamba\", \"Santa Cruz\"],\n",
    "    \"Argentina\": [\"Argentina\", \"Buenos Aires\", \"Córdoba\"],\n",
    "    \"Perú\": [\"Perú\", \"Lima\", \"Cusco\"]\n",
    "}\n",
    "\n",
    "def consulta_pais(pais, terminos, dias=30, maxrecords=250):\n",
    "    f = Filters(\n",
    "        keyword=\" OR \".join(terminos),\n",
    "        timelimit=f\"{dias}d\",\n",
    "        numrecords=maxrecords,\n",
    "        mode=\"ArtList\",\n",
    "        translation=\"es\"  # prioriza español\n",
    "    )\n",
    "    df = gd.article_search(f)\n",
    "    if df is None or df.empty:\n",
    "        return pd.DataFrame()\n",
    "    df[\"pais_consulta\"] = pais\n",
    "    return df\n",
    "\n",
    "dfs = [consulta_pais(p, ts) for p, ts in PAISES.items()]\n",
    "raw = pd.concat([d for d in dfs if not d.empty], ignore_index=True).drop_duplicates(subset=[\"url\"])\n",
    "raw.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee02b62",
   "metadata": {},
   "source": [
    "## Limpieza y pipeline de spaCy (lematización ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from unidecode import unidecode\n",
    "nlp = spacy.load(\"es_core_news_md\")  # :contentReference[oaicite:14]{index=14}\n",
    "\n",
    "def limpiar(t):\n",
    "    t = re.sub(r\"http\\\\S+|www\\\\.\\\\S+\", \" \", t)\n",
    "    t = re.sub(r\"[@#]\\\\w+\", \" \", t)\n",
    "    t = re.sub(r\"\\\\s+\", \" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def normalizar_es(t):\n",
    "    t = t.lower()\n",
    "    t = limpiar(t)\n",
    "    t = unidecode(t)  # opcional: quitar acentos para vocabularios simples\n",
    "    return t\n",
    "\n",
    "raw[\"texto\"] = (raw[\"title\"]  # o 'seendate'/'language' según columnas disponibles del cliente\n",
    "                .fillna(\"\")\n",
    "                .map(normalizar_es))\n",
    "\n",
    "def lemas_es(texto):\n",
    "    doc = nlp(texto)\n",
    "    return [tok.lemma_ for tok in doc \n",
    "            if not tok.is_stop and not tok.is_punct and tok.is_alpha and len(tok) > 2]\n",
    "\n",
    "raw[\"lemmas\"] = raw[\"texto\"].map(lemas_es)\n",
    "raw[[\"pais_consulta\",\"texto\",\"lemmas\"]].head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb20c65",
   "metadata": {},
   "source": [
    "## Frecuencias y n-gramas (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f42be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer  # :contentReference[oaicite:15]{index=15}\n",
    "def top_tokens(df, n=15, ngram=(1,1)):\n",
    "    corpus = df[\"texto\"].tolist()\n",
    "    vec = CountVectorizer(ngram_range=ngram, min_df=2)\n",
    "    X = vec.fit_transform(corpus)\n",
    "    freqs = X.sum(axis=0).A1\n",
    "    vocab = vec.get_feature_names_out()\n",
    "    out = (pd.DataFrame({\"token\": vocab, \"freq\": freqs})\n",
    "             .sort_values(\"freq\", ascending=False)\n",
    "             .head(n))\n",
    "    return out\n",
    "\n",
    "top_por_pais = {}\n",
    "for p in raw[\"pais_consulta\"].unique():\n",
    "    top_por_pais[p] = top_tokens(raw.query(\"pais_consulta == @p\"), n=15, ngram=(1,2))\n",
    "top_por_pais[\"Bolivia\"].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf474418",
   "metadata": {},
   "source": [
    "## Sentimiento (dos enfoques)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e282c4f8",
   "metadata": {},
   "source": [
    "### Diccionario mínimo (reglas transparentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ca25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEX_POS = {\"progreso\",\"exito\",\"crecer\",\"bueno\",\"positivo\",\"mejora\",\"ganar\",\"feliz\",\"beneficio\",\"lider\"}\n",
    "LEX_NEG = {\"crisis\",\"corrupcion\",\"malo\",\"negativo\",\"caida\",\"violencia\",\"perdida\",\"protesta\",\"denuncia\",\"riesgo\"}\n",
    "\n",
    "def score_lex(texto):\n",
    "    toks = set(texto.split())\n",
    "    return len(toks & LEX_POS) - len(toks & LEX_NEG)\n",
    "\n",
    "raw[\"sent_lex\"] = raw[\"texto\"].map(score_lex)\n",
    "raw.groupby(\"pais_consulta\")[\"sent_lex\"].mean().round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19603ef",
   "metadata": {},
   "source": [
    "### Modelo ML para español con pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a29cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import create_analyzer  # :contentReference[oaicite:17]{index=17}\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")  # descarga modelo BETO/roBERTuito\n",
    "\n",
    "def sent_label(texto):\n",
    "    out = analyzer.predict(texto)\n",
    "    return out.output  # \"POS\", \"NEG\", \"NEU\"\n",
    "\n",
    "raw[\"sent_ml\"] = raw[\"texto\"].map(sent_label)\n",
    "sent_resumen = (raw.groupby([\"pais_consulta\",\"sent_ml\"])\n",
    "                   .size().unstack(fill_value=0))\n",
    "sent_resumen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e4eb1",
   "metadata": {},
   "source": [
    "## Topic modelling (LDA con scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6736b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation  # LDA clásico  # :contentReference[oaicite:18]{index=18}\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Usar TF o TF-IDF con n-gramas; para LDA suele preferirse conteos (CountVectorizer)\n",
    "vec = CountVectorizer(min_df=3, max_df=0.9, ngram_range=(1,2))\n",
    "X = vec.fit_transform(raw[\"texto\"])\n",
    "lda = LatentDirichletAllocation(n_components=8, learning_method=\"batch\", random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "def mostrar_top_palabras(modelo, vocab, n_top=10):\n",
    "    for i, comp in enumerate(modelo.components_):\n",
    "        top_ids = comp.argsort()[-n_top:][::-1]\n",
    "        palabras = [vocab[t] for t in top_ids]\n",
    "        print(f\"Tópico {i:02d}: \", \", \".join(palabras))\n",
    "\n",
    "mostrar_top_palabras(lda, vec.get_feature_names_out(), n_top=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
