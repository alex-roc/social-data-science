{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb989a5",
   "metadata": {},
   "source": [
    "# Introducci√≥n a spaCy: Procesamiento de Lenguaje Natural\n",
    "\n",
    "## ¬øQu√© es spaCy?\n",
    "\n",
    "**spaCy** es una biblioteca de Python para procesamiento avanzado de lenguaje natural (NLP). Es r√°pida, eficiente y perfecta para aplicaciones de producci√≥n. Con spaCy puedes:\n",
    "\n",
    "- Analizar texto y extraer informaci√≥n ling√º√≠stica\n",
    "- Identificar entidades nombradas (personas, lugares, organizaciones)\n",
    "- Analizar la estructura gramatical de las oraciones\n",
    "- Trabajar con m√∫ltiples idiomas\n",
    "\n",
    "## Principales Estructuras de Datos en spaCy\n",
    "\n",
    "spaCy tiene tres estructuras de datos principales:\n",
    "\n",
    "1. **Doc**: Representa un documento de texto completo\n",
    "2. **Token**: Representa una palabra individual o s√≠mbolo\n",
    "3. **Span**: Representa una secuencia de tokens (como una frase o entidad)\n",
    "\n",
    "En este ejercicio aprenderemos a:\n",
    "- Procesar texto con spaCy\n",
    "- Analizar tokens individuales\n",
    "- Extraer informaci√≥n ling√º√≠stica\n",
    "- Identificar entidades nombradas\n",
    "\n",
    "¬°Comencemos con ejemplos pr√°cticos usando textos sencillos! üó£Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a49b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero importamos spaCy\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Verificamos la instalaci√≥n de spaCy\n",
    "print(f\"Versi√≥n de spaCy: {spacy.__version__}\")\n",
    "print(\"¬°spaCy importado correctamente! üöÄ\")\n",
    "print()\n",
    "\n",
    "# Nota: Si no tienes el modelo espa√±ol instalado, ejecuta en terminal:\n",
    "# python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f152323",
   "metadata": {},
   "source": [
    "## 1. Cargar Modelo y Crear Documento (Doc)\n",
    "\n",
    "Un **Doc** es la estructura principal de spaCy que contiene el texto procesado con toda la informaci√≥n ling√º√≠stica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a35f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo de spaCy para espa√±ol\n",
    "# Si obtienes error, instala el modelo con: python -m spacy download es_core_news_sm\n",
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    print(\"‚úÖ Modelo espa√±ol cargado correctamente\")\n",
    "except OSError:\n",
    "    print(\"‚ùå Error: Modelo espa√±ol no encontrado.\")\n",
    "    print(\"Instala el modelo con: python -m spacy download es_core_news_sm\")\n",
    "    # Para este ejercicio, usaremos el modelo en ingl√©s como alternativa\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"üìù Usando modelo en ingl√©s como alternativa\")\n",
    "\n",
    "print(f\"Modelo cargado: {nlp.meta['name']}\")\n",
    "print(f\"Idioma: {nlp.meta['lang']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuestro primer documento (Doc) con texto sencillo\n",
    "texto_ejemplo = \"Mar√≠a vive en Madrid y trabaja en Google. Le gusta leer libros de ciencia ficci√≥n.\"\n",
    "\n",
    "# Procesar el texto con spaCy\n",
    "doc = nlp(texto_ejemplo)\n",
    "\n",
    "print(\"üìÑ Documento creado:\")\n",
    "print(f\"Texto original: {doc.text}\")\n",
    "print(f\"Tipo de objeto: {type(doc)}\")\n",
    "print(f\"Longitud (n√∫mero de tokens): {len(doc)}\")\n",
    "print()\n",
    "\n",
    "# Informaci√≥n b√°sica del documento\n",
    "print(\"üîç Informaci√≥n del documento:\")\n",
    "print(f\"¬øTiene entidades nombradas? {len(doc.ents) > 0}\")\n",
    "print(f\"N√∫mero de oraciones: {len(list(doc.sents))}\")\n",
    "print(f\"Idioma detectado: {doc.lang_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f680012c",
   "metadata": {},
   "source": [
    "## 2. Tokens - Analizando Palabras Individuales\n",
    "\n",
    "Un **Token** representa cada palabra, signo de puntuaci√≥n o elemento del texto. Cada token contiene mucha informaci√≥n ling√º√≠stica √∫til."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar tokens individuales\n",
    "print(\"üî§ An√°lisis de Tokens:\")\n",
    "print(\"Token\\t\\tLema\\t\\tPOS\\t\\tTipo\\t\\t¬øEs palabra?\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<10}\\t{token.lemma_:<10}\\t{token.pos_:<8}\\t{token.tag_:<10}\\t{token.is_alpha}\")\n",
    "\n",
    "print()\n",
    "print(\"üìñ Explicaci√≥n de las columnas:\")\n",
    "print(\"- Token: La palabra tal como aparece en el texto\")\n",
    "print(\"- Lema: La forma base de la palabra (infinitivo, singular, etc.)\")\n",
    "print(\"- POS: Parte del discurso (sustantivo, verbo, adjetivo, etc.)\")\n",
    "print(\"- Tipo: Etiqueta espec√≠fica del POS\")\n",
    "print(\"- ¬øEs palabra?: True si es una palabra alfab√©tica (no puntuaci√≥n)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2377d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar diferentes tipos de tokens\n",
    "print(\"üìù Filtros de Tokens:\")\n",
    "\n",
    "# Solo palabras (sin puntuaci√≥n)\n",
    "palabras = [token.text for token in doc if token.is_alpha]\n",
    "print(f\"Solo palabras: {palabras}\")\n",
    "\n",
    "# Solo sustantivos\n",
    "sustantivos = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "print(f\"Sustantivos: {sustantivos}\")\n",
    "\n",
    "# Solo verbos\n",
    "verbos = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "print(f\"Verbos: {verbos}\")\n",
    "\n",
    "# Palabras importantes (no stop words)\n",
    "palabras_importantes = [token.text for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(f\"Palabras importantes: {palabras_importantes}\")\n",
    "\n",
    "print()\n",
    "print(\"üéØ An√°lisis espec√≠fico de algunos tokens:\")\n",
    "for token in doc:\n",
    "    if token.text in [\"Mar√≠a\", \"Madrid\", \"Google\"]:\n",
    "        print(f\"'{token.text}' -> Lema: {token.lemma_}, POS: {token.pos_}, ¬øStop word?: {token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937e668",
   "metadata": {},
   "source": [
    "## 3. Entidades Nombradas (Named Entities)\n",
    "\n",
    "Las **entidades nombradas** son elementos espec√≠ficos del texto como personas, lugares, organizaciones, fechas, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3246af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar entidades nombradas\n",
    "print(\"üè∑Ô∏è Entidades Nombradas encontradas:\")\n",
    "if doc.ents:\n",
    "    print(\"Entidad\\t\\tTipo\\t\\tDescripci√≥n\")\n",
    "    print(\"-\" * 50)\n",
    "    for ent in doc.ents:\n",
    "        print(f\"{ent.text:<15}\\t{ent.label_:<10}\\t{spacy.explain(ent.label_)}\")\n",
    "else:\n",
    "    print(\"No se encontraron entidades nombradas en este texto\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Crear un texto m√°s rico en entidades para demostrar mejor\n",
    "texto_rico = \"\"\"\n",
    "Juan P√©rez naci√≥ el 15 de marzo de 1990 en Barcelona, Espa√±a. \n",
    "Trabaja en Microsoft desde 2020 y gana 50000 euros al a√±o. \n",
    "Su tel√©fono es +34 123 456 789 y su email es juan@email.com.\n",
    "\"\"\"\n",
    "\n",
    "doc_rico = nlp(texto_rico)\n",
    "\n",
    "print(\"üåü Ejemplo con texto m√°s rico en entidades:\")\n",
    "print(f\"Texto: {texto_rico.strip()}\")\n",
    "print()\n",
    "print(\"Entidades encontradas:\")\n",
    "print(\"Entidad\\t\\tTipo\\t\\tDescripci√≥n\")\n",
    "print(\"-\" * 50)\n",
    "for ent in doc_rico.ents:\n",
    "    print(f\"{ent.text:<15}\\t{ent.label_:<10}\\t{spacy.explain(ent.label_) or 'Sin descripci√≥n'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924e4685",
   "metadata": {},
   "source": [
    "## 4. Spans y Segmentaci√≥n de Oraciones\n",
    "\n",
    "Un **Span** es una secuencia de tokens. Las oraciones son un tipo especial de Span que spaCy identifica autom√°ticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1ed4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trabajar con oraciones (sentences)\n",
    "texto_oraciones = \"\"\"\n",
    "Hola, me llamo Ana y tengo 25 a√±os. Vivo en Madrid con mi gato Felix. \n",
    "Me gusta cocinar pasta italiana. ¬øCu√°l es tu comida favorita? \n",
    "Mi restaurante preferido est√° en el centro de la ciudad.\n",
    "\"\"\"\n",
    "\n",
    "doc_oraciones = nlp(texto_oraciones)\n",
    "\n",
    "print(\"üìù Segmentaci√≥n de Oraciones:\")\n",
    "print(f\"N√∫mero total de oraciones: {len(list(doc_oraciones.sents))}\")\n",
    "print()\n",
    "\n",
    "for i, sent in enumerate(doc_oraciones.sents, 1):\n",
    "    print(f\"Oraci√≥n {i}: {sent.text.strip()}\")\n",
    "    print(f\"  - Tokens: {len(sent)}\")\n",
    "    print(f\"  - Palabras: {len([token for token in sent if token.is_alpha])}\")\n",
    "    print()\n",
    "\n",
    "# Crear spans personalizados\n",
    "print(\"üéØ Trabajando con Spans personalizados:\")\n",
    "# Obtener las primeras 3 palabras\n",
    "span_ejemplo = doc_oraciones[:3]\n",
    "print(f\"Span de las primeras 3 palabras: '{span_ejemplo.text}'\")\n",
    "print(f\"Tipo: {type(span_ejemplo)}\")\n",
    "print(f\"Tokens en el span: {[token.text for token in span_ejemplo]}\")\n",
    "\n",
    "# Span de una oraci√≥n espec√≠fica\n",
    "primera_oracion = list(doc_oraciones.sents)[0]\n",
    "print(f\"Primera oraci√≥n como span: '{primera_oracion.text.strip()}'\")\n",
    "print(f\"Ra√≠z de la oraci√≥n: '{primera_oracion.root.text}' (POS: {primera_oracion.root.pos_})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f354a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de dependencias sint√°cticas (sintaxis b√°sica)\n",
    "print(\"üå≥ An√°lisis de Dependencias Sint√°cticas:\")\n",
    "oraci√≥n_simple = nlp(\"El gato come pescado.\")\n",
    "\n",
    "print(f\"Oraci√≥n: {oraci√≥n_simple.text}\")\n",
    "print(\"Token\\t\\tDependencia\\t\\tJefe\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for token in oraci√≥n_simple:\n",
    "    print(f\"{token.text:<10}\\t{token.dep_:<15}\\t{token.head.text}\")\n",
    "\n",
    "print()\n",
    "print(\"üìä Creando un DataFrame con informaci√≥n de tokens:\")\n",
    "\n",
    "# Crear un DataFrame con la informaci√≥n de los tokens\n",
    "datos_tokens = []\n",
    "for token in doc_oraciones:\n",
    "    if token.is_alpha:  # Solo palabras, no puntuaci√≥n\n",
    "        datos_tokens.append({\n",
    "            'palabra': token.text,\n",
    "            'lema': token.lemma_,\n",
    "            'pos': token.pos_,\n",
    "            'es_stop_word': token.is_stop,\n",
    "            'dependencia': token.dep_,\n",
    "            'longitud': len(token.text)\n",
    "        })\n",
    "\n",
    "df_tokens = pd.DataFrame(datos_tokens)\n",
    "print(\"Muestra del DataFrame de tokens:\")\n",
    "print(df_tokens.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c41cfc",
   "metadata": {},
   "source": [
    "## 5. ¬°Tu turno! Ejercicios Pr√°cticos üöÄ\n",
    "\n",
    "Ahora es momento de que practiques con los conceptos de spaCy. Aqu√≠ tienes algunos ejercicios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 1: Analiza tu propio texto\n",
    "print(\"üìù EJERCICIO 1 - Analiza este texto sobre deportes:\")\n",
    "\n",
    "texto_deportes = \"\"\"\n",
    "El Real Madrid gan√≥ el partido por 3-1 contra el Barcelona el domingo pasado. \n",
    "Cristiano Ronaldo marc√≥ dos goles incre√≠bles en el Santiago Bernab√©u. \n",
    "El entrenador Zinedine Zidane estaba muy contento con el resultado.\n",
    "\"\"\"\n",
    "\n",
    "# Tu c√≥digo aqu√≠: procesa el texto con spaCy\n",
    "doc_deportes = nlp(texto_deportes)\n",
    "\n",
    "print(f\"Texto analizado: {doc_deportes.text}\")\n",
    "print()\n",
    "\n",
    "# Responde estas preguntas:\n",
    "print(\"‚ùì Preguntas para resolver:\")\n",
    "\n",
    "# A) ¬øCu√°ntas oraciones hay?\n",
    "num_oraciones = len(list(doc_deportes.sents))\n",
    "print(f\"A) N√∫mero de oraciones: {num_oraciones}\")\n",
    "\n",
    "# B) ¬øQu√© entidades nombradas encontraste?\n",
    "print(\"B) Entidades nombradas:\")\n",
    "for ent in doc_deportes.ents:\n",
    "    print(f\"   - {ent.text} ({ent.label_})\")\n",
    "\n",
    "# C) ¬øCu√°les son los verbos en el texto?\n",
    "verbos = [token.text for token in doc_deportes if token.pos_ == \"VERB\"]\n",
    "print(f\"C) Verbos encontrados: {verbos}\")\n",
    "\n",
    "# D) ¬øCu√°ntas palabras √∫nicas hay? (sin repeticiones)\n",
    "palabras_unicas = set([token.lemma_.lower() for token in doc_deportes if token.is_alpha])\n",
    "print(f\"D) Palabras √∫nicas: {len(palabras_unicas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99112ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 2: Comparar textos\n",
    "print(\"üîÑ EJERCICIO 2 - Compara estos dos textos:\")\n",
    "\n",
    "texto1 = \"Me encanta la pizza italiana con pepperoni y queso.\"\n",
    "texto2 = \"Odio la comida r√°pida, prefiero ensaladas saludables.\"\n",
    "\n",
    "doc1 = nlp(texto1)\n",
    "doc2 = nlp(texto2)\n",
    "\n",
    "print(f\"Texto 1: {texto1}\")\n",
    "print(f\"Texto 2: {texto2}\")\n",
    "print()\n",
    "\n",
    "# An√°lisis comparativo\n",
    "print(\"üìä An√°lisis comparativo:\")\n",
    "\n",
    "# Palabras importantes (sin stop words)\n",
    "palabras1 = [token.lemma_ for token in doc1 if not token.is_stop and token.is_alpha]\n",
    "palabras2 = [token.lemma_ for token in doc2 if not token.is_stop and token.is_alpha]\n",
    "\n",
    "print(f\"Palabras clave texto 1: {palabras1}\")\n",
    "print(f\"Palabras clave texto 2: {palabras2}\")\n",
    "\n",
    "# ¬øTienen alguna palabra en com√∫n?\n",
    "palabras_comunes = set(palabras1) & set(palabras2)\n",
    "print(f\"Palabras en com√∫n: {palabras_comunes if palabras_comunes else 'Ninguna'}\")\n",
    "\n",
    "# An√°lisis de sentimientos b√°sico (contando palabras positivas/negativas)\n",
    "palabras_positivas = [\"encanta\", \"gusta\", \"prefiero\", \"saludables\"]\n",
    "palabras_negativas = [\"odio\", \"malo\", \"terrible\", \"horrible\"]\n",
    "\n",
    "sentimiento1 = sum(1 for token in doc1 if token.lemma_ in palabras_positivas) - sum(1 for token in doc1 if token.lemma_ in palabras_negativas)\n",
    "sentimiento2 = sum(1 for token in doc2 if token.lemma_ in palabras_positivas) - sum(1 for token in doc2 if token.lemma_ in palabras_negativas)\n",
    "\n",
    "print(f\"Sentimiento texto 1: {sentimiento1} (positivo si > 0)\")\n",
    "print(f\"Sentimiento texto 2: {sentimiento2} (positivo si > 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f951df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 3: Extractor de informaci√≥n autom√°tico\n",
    "print(\"üîç EJERCICIO 3 - Extractor de informaci√≥n autom√°tico\")\n",
    "\n",
    "# Funci√≥n para extraer informaci√≥n clave de un texto\n",
    "def extraer_informacion(texto):\n",
    "    doc = nlp(texto)\n",
    "    \n",
    "    info = {\n",
    "        'num_oraciones': len(list(doc.sents)),\n",
    "        'num_palabras': len([token for token in doc if token.is_alpha]),\n",
    "        'personas': [],\n",
    "        'lugares': [],\n",
    "        'organizaciones': [],\n",
    "        'sustantivos': [],\n",
    "        'adjetivos': []\n",
    "    }\n",
    "    \n",
    "    # Extraer entidades nombradas\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"PER\"]:\n",
    "            info['personas'].append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:  # GPE = Geopolitical entity, LOC = Location\n",
    "            info['lugares'].append(ent.text)\n",
    "        elif ent.label_ in [\"ORG\"]:\n",
    "            info['organizaciones'].append(ent.text)\n",
    "    \n",
    "    # Extraer sustantivos y adjetivos importantes\n",
    "    for token in doc:\n",
    "        if token.pos_ == \"NOUN\" and not token.is_stop:\n",
    "            info['sustantivos'].append(token.lemma_)\n",
    "        elif token.pos_ == \"ADJ\" and not token.is_stop:\n",
    "            info['adjetivos'].append(token.lemma_)\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Probar con diferentes textos\n",
    "textos_prueba = [\n",
    "    \"Apple Inc. fue fundada por Steve Jobs en California.\",\n",
    "    \"Mar√≠a viaj√≥ a Par√≠s el a√±o pasado con su hermana.\",\n",
    "    \"El nuevo iPhone es incre√≠blemente r√°pido y elegante.\"\n",
    "]\n",
    "\n",
    "for i, texto in enumerate(textos_prueba, 1):\n",
    "    print(f\"\\n--- Texto {i} ---\")\n",
    "    print(f\"Texto: {texto}\")\n",
    "    info = extraer_informacion(texto)\n",
    "    \n",
    "    for clave, valor in info.items():\n",
    "        if valor:  # Solo mostrar si hay contenido\n",
    "            print(f\"{clave.replace('_', ' ').title()}: {valor}\")\n",
    "\n",
    "print(\"\\nüí° ¬°Tu turno! Modifica la funci√≥n 'extraer_informacion' para:\")\n",
    "print(\"   - Agregar detecci√≥n de fechas\")\n",
    "print(\"   - Contar palabras largas (>6 caracteres)\")\n",
    "print(\"   - Identificar el tema principal del texto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6268b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EJERCICIO 4: Espacio libre para experimentar\n",
    "print(\"üéØ EJERCICIO 4 - Espacio libre para practicar\")\n",
    "print(\"Usa este espacio para experimentar con tus propios textos\")\n",
    "print()\n",
    "\n",
    "# Ideas para practicar:\n",
    "print(\"Ideas para experimentar:\")\n",
    "print(\"1. Analizar un texto de una noticia\")\n",
    "print(\"2. Comparar textos en diferentes idiomas\")\n",
    "print(\"3. Crear un contador de palabras m√°s inteligente\")\n",
    "print(\"4. Hacer an√°lisis de sentimientos b√°sico\")\n",
    "print(\"5. Extraer informaci√≥n de biograf√≠as\")\n",
    "print()\n",
    "\n",
    "# Ejemplo de texto para experimentar\n",
    "mi_texto = \"\"\"\n",
    "Escribe aqu√≠ tu propio texto para analizar. \n",
    "Puede ser una noticia, una historia personal, o cualquier cosa que te interese.\n",
    "¬°spaCy te ayudar√° a extraer informaci√≥n valiosa!\n",
    "\"\"\"\n",
    "\n",
    "# Tu c√≥digo de experimentaci√≥n aqu√≠:\n",
    "# (Descomenta y modifica estas l√≠neas)\n",
    "\n",
    "# doc_mi_texto = nlp(mi_texto)\n",
    "# print(f\"Mi texto tiene {len(list(doc_mi_texto.sents))} oraciones\")\n",
    "# print(f\"Entidades encontradas: {[(ent.text, ent.label_) for ent in doc_mi_texto.ents]}\")\n",
    "\n",
    "print(\"¬°Adelante, experimenta con spaCy!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
