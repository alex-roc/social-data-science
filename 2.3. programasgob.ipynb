{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565f4020",
   "metadata": {},
   "source": [
    "# An√°lisis de Programas de Gobierno 2025 con SpaCy\n",
    "\n",
    "Este notebook contiene un an√°lisis completo de los programas de gobierno de diferentes partidos pol√≠ticos para las elecciones de 2025 en Bolivia, utilizando t√©cnicas de procesamiento de lenguaje natural con SpaCy.\n",
    "\n",
    "## Objetivos del an√°lisis:\n",
    "1. **An√°lisis de frecuencias**: Palabras y entidades m√°s comunes\n",
    "2. **An√°lisis de sentimientos**: Usando diccionarios y modelos transformer\n",
    "3. **Topic modeling**: Identificaci√≥n de temas principales\n",
    "4. **Comparaci√≥n entre partidos**: Similitudes y diferencias\n",
    "\n",
    "## Partidos analizados:\n",
    "- ADN, AP, FP, Libre, MAS, MORENA, NGP, PDC, SUMATE, UNIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b385ff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (3887913243.py, line 31)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\"‚ùå Error instalando es_core_news_sm: {e}\")ore_news_sm: {e}\")_core_news_sm: {e}\")core_news_sm\")md instalado\")\u001b[39m\n                                                                                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# Verificar que todo est√° instalado correctamente\n",
    "import spacy\n",
    "import transformers\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as est√°n disponibles\")\n",
    "\n",
    "# Verificar modelo de SpaCy\n",
    "try:\n",
    "    nlp = spacy.load('es_core_news_md')\n",
    "    print(\"‚úÖ Modelo de SpaCy es_core_news_md cargado\")\n",
    "except OSError:\n",
    "    print(\"‚ùå Error: Modelo de SpaCy no encontrado\")\n",
    "\n",
    "print(\"‚úÖ Todo listo para el an√°lisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd563df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librer√≠as\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from collections import Counter, defaultdict\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Para an√°lisis de sentimientos\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Para topic modeling\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim import corpora, models\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo de spaCy\n",
    "nlp = spacy.load('es_core_news_md')\n",
    "\n",
    "# Configurar analizador de sentimientos VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Cargar modelo transformer para sentimientos en espa√±ol\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\", \n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
    "    tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Modelos cargados correctamente\")\n",
    "print(f\"SpaCy model: {nlp.meta['name']} - {nlp.meta['version']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e5b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar y procesar los documentos\n",
    "def load_government_programs():\n",
    "    \"\"\"\n",
    "    Carga todos los programas de gobierno desde la carpeta data/2025\n",
    "    \"\"\"\n",
    "    data_path = Path('/Users/alexojeda/dev/social-data-science/data/2025')\n",
    "    programs = {}\n",
    "    metadata = {}\n",
    "    \n",
    "    for file_path in data_path.glob('*.txt'):\n",
    "        party_name = file_path.stem\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Extraer metadatos del header\n",
    "        lines = content.split('\\n')\n",
    "        party_info = {}\n",
    "        content_start = 0\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if line.startswith('partido:'):\n",
    "                party_info['partido_completo'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('candidato_presidente:'):\n",
    "                party_info['candidato_presidente'] = line.split(':', 1)[1].strip()\n",
    "            elif line.startswith('candidato_vicepresidente:'):\n",
    "                party_info['candidato_vicepresidente'] = line.split(':', 1)[1].strip()\n",
    "            elif line.strip() == '---' and i > 0:\n",
    "                content_start = i + 1\n",
    "                break\n",
    "        \n",
    "        # Extraer el contenido del programa (sin metadatos)\n",
    "        program_content = '\\n'.join(lines[content_start:]).strip()\n",
    "        \n",
    "        programs[party_name] = program_content\n",
    "        metadata[party_name] = party_info\n",
    "    \n",
    "    return programs, metadata\n",
    "\n",
    "# Cargar los datos\n",
    "programs, metadata = load_government_programs()\n",
    "\n",
    "print(f\"‚úÖ Cargados {len(programs)} programas de gobierno\")\n",
    "print(\"\\nPartidos disponibles:\")\n",
    "for party, info in metadata.items():\n",
    "    print(f\"- {party}: {info.get('partido_completo', 'N/A')}\")\n",
    "    print(f\"  Candidato: {info.get('candidato_presidente', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2669bd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n de preprocesamiento con spaCy\n",
    "def preprocess_text(text, remove_stopwords=True, lemmatize=True, pos_filter=None):\n",
    "    \"\"\"\n",
    "    Preprocesa texto usando spaCy\n",
    "    \n",
    "    Args:\n",
    "        text: Texto a procesar\n",
    "        remove_stopwords: Si remover stopwords\n",
    "        lemmatize: Si lematizar\n",
    "        pos_filter: Lista de POS tags a mantener (ej: ['NOUN', 'ADJ', 'VERB'])\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # Filtrar tokens no deseados\n",
    "        if token.is_alpha and len(token.text) > 2:\n",
    "            if remove_stopwords and token.is_stop:\n",
    "                continue\n",
    "            if pos_filter and token.pos_ not in pos_filter:\n",
    "                continue\n",
    "            \n",
    "            # Usar lema o texto original\n",
    "            word = token.lemma_.lower() if lemmatize else token.text.lower()\n",
    "            tokens.append(word)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Funci√≥n para extraer entidades nombradas\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extrae entidades nombradas del texto\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        entities.append({\n",
    "            'text': ent.text,\n",
    "            'label': ent.label_,\n",
    "            'description': spacy.explain(ent.label_)\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "print(\"‚úÖ Funciones de preprocesamiento definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3557c6a",
   "metadata": {},
   "source": [
    "## 1. An√°lisis de Frecuencias\n",
    "\n",
    "Analicemos las palabras m√°s frecuentes en los programas de gobierno y las entidades nombradas m√°s mencionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5346a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de frecuencias por partido\n",
    "frequency_analysis = {}\n",
    "entity_analysis = {}\n",
    "all_tokens = []\n",
    "all_entities = []\n",
    "\n",
    "for party, program in programs.items():\n",
    "    # An√°lisis de frecuencias\n",
    "    tokens = preprocess_text(program, pos_filter=['NOUN', 'ADJ', 'VERB'])\n",
    "    frequency_analysis[party] = Counter(tokens)\n",
    "    all_tokens.extend(tokens)\n",
    "    \n",
    "    # An√°lisis de entidades\n",
    "    entities = extract_entities(program)\n",
    "    entity_analysis[party] = entities\n",
    "    all_entities.extend(entities)\n",
    "    \n",
    "    print(f\"üìä {party}: {len(tokens)} tokens procesados, {len(entities)} entidades encontradas\")\n",
    "\n",
    "# Frecuencias globales\n",
    "global_frequencies = Counter(all_tokens)\n",
    "print(f\"\\nüìà Total: {len(all_tokens)} tokens, {len(set(all_tokens))} √∫nicos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5e7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de palabras m√°s frecuentes globalmente\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "\n",
    "# Top 20 palabras m√°s frecuentes\n",
    "top_words = global_frequencies.most_common(20)\n",
    "words, counts = zip(*top_words)\n",
    "\n",
    "axes[0,0].barh(range(len(words)), counts)\n",
    "axes[0,0].set_yticks(range(len(words)))\n",
    "axes[0,0].set_yticklabels(words)\n",
    "axes[0,0].set_title('Top 20 Palabras M√°s Frecuentes (Global)', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Frecuencia')\n",
    "axes[0,0].invert_yaxis()\n",
    "\n",
    "# WordCloud global\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                     max_words=100, colormap='viridis').generate_from_frequencies(global_frequencies)\n",
    "axes[0,1].imshow(wordcloud, interpolation='bilinear')\n",
    "axes[0,1].axis('off')\n",
    "axes[0,1].set_title('WordCloud - Todas las Palabras', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Distribuci√≥n de longitud de programas\n",
    "program_lengths = [len(preprocess_text(program)) for program in programs.values()]\n",
    "parties = list(programs.keys())\n",
    "\n",
    "axes[1,0].bar(parties, program_lengths, color=sns.color_palette(\"husl\", len(parties)))\n",
    "axes[1,0].set_title('Longitud de Programas por Partido', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_ylabel('N√∫mero de tokens')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# An√°lisis de entidades m√°s comunes\n",
    "entity_counter = Counter([ent['text'].lower() for ent in all_entities if len(ent['text']) > 2])\n",
    "top_entities = entity_counter.most_common(15)\n",
    "if top_entities:\n",
    "    ent_names, ent_counts = zip(*top_entities)\n",
    "    axes[1,1].barh(range(len(ent_names)), ent_counts)\n",
    "    axes[1,1].set_yticks(range(len(ent_names)))\n",
    "    axes[1,1].set_yticklabels(ent_names)\n",
    "    axes[1,1].set_title('Top 15 Entidades M√°s Mencionadas', fontsize=14, fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Frecuencia')\n",
    "    axes[1,1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c93313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n de palabras m√°s frecuentes por partido\n",
    "fig, axes = plt.subplots(2, 5, figsize=(25, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (party, freq_counter) in enumerate(frequency_analysis.items()):\n",
    "    if i < len(axes):\n",
    "        top_10 = freq_counter.most_common(10)\n",
    "        if top_10:\n",
    "            words, counts = zip(*top_10)\n",
    "            axes[i].barh(range(len(words)), counts)\n",
    "            axes[i].set_yticks(range(len(words)))\n",
    "            axes[i].set_yticklabels(words, fontsize=8)\n",
    "            axes[i].set_title(f'{party}\\n({sum(freq_counter.values())} tokens)', \n",
    "                            fontsize=10, fontweight='bold')\n",
    "            axes[i].invert_yaxis()\n",
    "            axes[i].tick_params(axis='x', labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Top 10 Palabras por Partido Pol√≠tico', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caeaf65",
   "metadata": {},
   "source": [
    "## 2. An√°lisis de Sentimientos\n",
    "\n",
    "Analizaremos los sentimientos de los programas usando m√∫ltiples enfoques:\n",
    "1. **VADER**: Diccionario especializado en sentimientos\n",
    "2. **TextBlob**: An√°lisis de polaridad y subjetividad\n",
    "3. **BERT Multilingual**: Modelo transformer pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c458be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para an√°lisis de sentimientos completo\n",
    "def analyze_sentiment_comprehensive(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Analiza sentimientos usando m√∫ltiples m√©todos\n",
    "    \"\"\"\n",
    "    # Dividir texto en chunks para modelos con l√≠mite de tokens\n",
    "    chunks = [text[i:i+max_length*4] for i in range(0, len(text), max_length*4)]\n",
    "    \n",
    "    results = {\n",
    "        'vader': {'compound': [], 'pos': [], 'neu': [], 'neg': []},\n",
    "        'textblob': {'polarity': [], 'subjectivity': []},\n",
    "        'bert': {'label': [], 'score': []}\n",
    "    }\n",
    "    \n",
    "    for chunk in chunks[:3]:  # Analizar m√°ximo 3 chunks por documento\n",
    "        if len(chunk.strip()) > 10:\n",
    "            # VADER\n",
    "            vader_scores = analyzer.polarity_scores(chunk)\n",
    "            for key in results['vader']:\n",
    "                results['vader'][key].append(vader_scores[key])\n",
    "            \n",
    "            # TextBlob\n",
    "            blob = TextBlob(chunk)\n",
    "            results['textblob']['polarity'].append(blob.sentiment.polarity)\n",
    "            results['textblob']['subjectivity'].append(blob.sentiment.subjectivity)\n",
    "            \n",
    "            # BERT (con manejo de errores)\n",
    "            try:\n",
    "                bert_result = sentiment_pipeline(chunk[:512])[0]\n",
    "                results['bert']['label'].append(bert_result['label'])\n",
    "                results['bert']['score'].append(bert_result['score'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error en BERT: {e}\")\n",
    "                results['bert']['label'].append('NEUTRAL')\n",
    "                results['bert']['score'].append(0.5)\n",
    "    \n",
    "    # Promediar resultados\n",
    "    final_results = {\n",
    "        'vader_compound': np.mean(results['vader']['compound']) if results['vader']['compound'] else 0,\n",
    "        'vader_positive': np.mean(results['vader']['pos']) if results['vader']['pos'] else 0,\n",
    "        'vader_neutral': np.mean(results['vader']['neu']) if results['vader']['neu'] else 0,\n",
    "        'vader_negative': np.mean(results['vader']['neg']) if results['vader']['neg'] else 0,\n",
    "        'textblob_polarity': np.mean(results['textblob']['polarity']) if results['textblob']['polarity'] else 0,\n",
    "        'textblob_subjectivity': np.mean(results['textblob']['subjectivity']) if results['textblob']['subjectivity'] else 0,\n",
    "        'bert_label': max(set(results['bert']['label']), key=results['bert']['label'].count) if results['bert']['label'] else 'NEUTRAL',\n",
    "        'bert_score': np.mean(results['bert']['score']) if results['bert']['score'] else 0.5\n",
    "    }\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de an√°lisis de sentimientos definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ac71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar an√°lisis de sentimientos para todos los partidos\n",
    "sentiment_results = {}\n",
    "\n",
    "print(\"üîç Analizando sentimientos...\")\n",
    "for party, program in programs.items():\n",
    "    print(f\"  Procesando {party}...\")\n",
    "    sentiment_results[party] = analyze_sentiment_comprehensive(program)\n",
    "    \n",
    "# Crear DataFrame con resultados\n",
    "sentiment_df = pd.DataFrame(sentiment_results).T\n",
    "sentiment_df['partido'] = sentiment_df.index\n",
    "sentiment_df = sentiment_df.reset_index(drop=True)\n",
    "\n",
    "print(\"\\nüìä Resultados de an√°lisis de sentimientos:\")\n",
    "print(sentiment_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e662b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del an√°lisis de sentimientos\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'VADER - Sentimiento Compuesto',\n",
    "        'TextBlob - Polaridad vs Subjetividad', \n",
    "        'VADER - Distribuci√≥n de Sentimientos',\n",
    "        'BERT - Clasificaci√≥n de Sentimientos'\n",
    "    ),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# VADER Compound Score\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=sentiment_df['partido'],\n",
    "        y=sentiment_df['vader_compound'],\n",
    "        name='VADER Compound',\n",
    "        marker_color='lightblue'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# TextBlob Scatter\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sentiment_df['textblob_polarity'],\n",
    "        y=sentiment_df['textblob_subjectivity'],\n",
    "        mode='markers+text',\n",
    "        text=sentiment_df['partido'],\n",
    "        textposition='top center',\n",
    "        marker=dict(size=12, color='red'),\n",
    "        name='TextBlob'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# VADER Distribution (stacked bar)\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=sentiment_df['partido'],\n",
    "        y=sentiment_df['vader_positive'],\n",
    "        name='Positivo',\n",
    "        marker_color='green'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=sentiment_df['partido'],\n",
    "        y=sentiment_df['vader_neutral'],\n",
    "        name='Neutral',\n",
    "        marker_color='gray'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=sentiment_df['partido'],\n",
    "        y=sentiment_df['vader_negative'],\n",
    "        name='Negativo',\n",
    "        marker_color='red'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# BERT Results\n",
    "bert_counts = sentiment_df['bert_label'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=bert_counts.index,\n",
    "        y=bert_counts.values,\n",
    "        name='BERT Classification',\n",
    "        marker_color='purple'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"An√°lisis de Sentimientos - Programas de Gobierno 2025\",\n",
    "    title_x=0.5,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=45, row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Polaridad\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Subjetividad\", row=1, col=2)\n",
    "fig.update_xaxes(tickangle=45, row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76738b0",
   "metadata": {},
   "source": [
    "## 3. Topic Modeling\n",
    "\n",
    "Utilizaremos Latent Dirichlet Allocation (LDA) para identificar los temas principales en los programas de gobierno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c248482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para topic modeling\n",
    "def prepare_documents_for_lda(programs):\n",
    "    \"\"\"\n",
    "    Prepara documentos para LDA eliminando palabras muy comunes y muy raras\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    doc_names = []\n",
    "    \n",
    "    for party, program in programs.items():\n",
    "        # Procesar texto con filtros m√°s estrictos\n",
    "        tokens = preprocess_text(program, pos_filter=['NOUN', 'ADJ'])\n",
    "        \n",
    "        # Filtrar palabras muy cortas o muy comunes en pol√≠tica\n",
    "        political_stopwords = {\n",
    "            'bolivia', 'boliviano', 'boliviana', 'pa√≠s', 'estado', 'gobierno', \n",
    "            'nacional', 'p√∫blico', 'social', 'econ√≥mico', 'pol√≠tica', 'pol√≠tico',\n",
    "            'pueblo', 'ciudadano', 'sociedad', 'desarrollo', 'gesti√≥n', 'proceso'\n",
    "        }\n",
    "        \n",
    "        filtered_tokens = [\n",
    "            token for token in tokens \n",
    "            if len(token) > 3 and token not in political_stopwords\n",
    "        ]\n",
    "        \n",
    "        documents.append(filtered_tokens)\n",
    "        doc_names.append(party)\n",
    "    \n",
    "    return documents, doc_names\n",
    "\n",
    "# Preparar documentos\n",
    "documents, doc_names = prepare_documents_for_lda(programs)\n",
    "\n",
    "# Crear diccionario y corpus para Gensim\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Filtrar extremos: palabras que aparecen en menos de 2 docs o m√°s del 50% de docs\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "\n",
    "print(f\"üìö Corpus preparado:\")\n",
    "print(f\"  - {len(documents)} documentos\")\n",
    "print(f\"  - {len(dictionary)} palabras √∫nicas\")\n",
    "print(f\"  - {sum(len(doc) for doc in corpus)} tokens totales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1de051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo LDA\n",
    "num_topics = 6  # N√∫mero de temas a identificar\n",
    "\n",
    "print(\"ü§ñ Entrenando modelo LDA...\")\n",
    "lda_model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    alpha='auto',\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Modelo LDA entrenado con {num_topics} temas\")\n",
    "\n",
    "# Extraer temas y sus palabras principales\n",
    "topics = []\n",
    "for idx in range(num_topics):\n",
    "    topic_words = lda_model.show_topic(idx, topn=10)\n",
    "    topics.append({\n",
    "        'topic_id': idx,\n",
    "        'words': [word for word, prob in topic_words],\n",
    "        'probabilities': [prob for word, prob in topic_words],\n",
    "        'description': ' + '.join([f\"{word}({prob:.3f})\" for word, prob in topic_words[:5]])\n",
    "    })\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Temas identificados:\")\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"\\nTema {i}: {topic['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar temas dominantes a cada documento\n",
    "document_topics = []\n",
    "for i, doc in enumerate(corpus):\n",
    "    doc_topics = lda_model.get_document_topics(doc)\n",
    "    # Obtener tema dominante\n",
    "    dominant_topic = max(doc_topics, key=lambda x: x[1])\n",
    "    \n",
    "    document_topics.append({\n",
    "        'partido': doc_names[i],\n",
    "        'dominant_topic_id': dominant_topic[0],\n",
    "        'dominant_topic_prob': dominant_topic[1],\n",
    "        'all_topics': doc_topics\n",
    "    })\n",
    "\n",
    "# Crear DataFrame de resultados\n",
    "topic_df = pd.DataFrame(document_topics)\n",
    "topic_df['dominant_topic_desc'] = topic_df['dominant_topic_id'].apply(\n",
    "    lambda x: topics[x]['description']\n",
    ")\n",
    "\n",
    "print(\"üìä Asignaci√≥n de temas por partido:\")\n",
    "print(topic_df[['partido', 'dominant_topic_id', 'dominant_topic_prob', 'dominant_topic_desc']].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8074ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n de Topic Modeling\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Distribuci√≥n de Temas por Partido',\n",
    "        'Probabilidad del Tema Dominante',\n",
    "        'Top Palabras por Tema',\n",
    "        'Matriz de Similitud de Temas'\n",
    "    ),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"bar\"}, {\"type\": \"heatmap\"}]]\n",
    ")\n",
    "\n",
    "# Distribuci√≥n de temas por partido\n",
    "topic_counts = topic_df['dominant_topic_id'].value_counts().sort_index()\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"Tema {i}\" for i in topic_counts.index],\n",
    "        y=topic_counts.values,\n",
    "        name='Distribuci√≥n de Temas',\n",
    "        marker_color='lightgreen'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Probabilidad del tema dominante por partido\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=topic_df['partido'],\n",
    "        y=topic_df['dominant_topic_prob'],\n",
    "        name='Probabilidad Tema Dominante',\n",
    "        marker_color='orange',\n",
    "        text=topic_df['dominant_topic_id'],\n",
    "        textposition='outside'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Top palabras del tema m√°s com√∫n\n",
    "most_common_topic = topic_counts.index[0]\n",
    "top_words = topics[most_common_topic]['words'][:10]\n",
    "top_probs = topics[most_common_topic]['probabilities'][:10]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=top_probs,\n",
    "        y=top_words,\n",
    "        orientation='h',\n",
    "        name=f'Tema {most_common_topic}',\n",
    "        marker_color='purple'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Matriz de similitud entre temas (usando palabras top)\n",
    "similarity_matrix = np.zeros((num_topics, num_topics))\n",
    "for i in range(num_topics):\n",
    "    for j in range(num_topics):\n",
    "        words_i = set(topics[i]['words'][:10])\n",
    "        words_j = set(topics[j]['words'][:10])\n",
    "        similarity = len(words_i.intersection(words_j)) / len(words_i.union(words_j))\n",
    "        similarity_matrix[i, j] = similarity\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=similarity_matrix,\n",
    "        x=[f\"Tema {i}\" for i in range(num_topics)],\n",
    "        y=[f\"Tema {i}\" for i in range(num_topics)],\n",
    "        colorscale='Viridis',\n",
    "        name='Similitud'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Topic Modeling - Programas de Gobierno 2025\",\n",
    "    title_x=0.5,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickangle=45, row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Palabras\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Probabilidad\", row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98776cd7",
   "metadata": {},
   "source": [
    "## 4. An√°lisis Comparativo y Conclusiones\n",
    "\n",
    "Realizaremos un an√°lisis comparativo final que combine todos los aspectos estudiados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524e1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis comparativo integral\n",
    "comparative_df = sentiment_df.copy()\n",
    "comparative_df = comparative_df.merge(\n",
    "    topic_df[['partido', 'dominant_topic_id', 'dominant_topic_prob']], \n",
    "    on='partido'\n",
    ")\n",
    "\n",
    "# Agregar estad√≠sticas de texto\n",
    "text_stats = []\n",
    "for party, program in programs.items():\n",
    "    tokens = preprocess_text(program)\n",
    "    entities = extract_entities(program)\n",
    "    \n",
    "    text_stats.append({\n",
    "        'partido': party,\n",
    "        'total_tokens': len(tokens),\n",
    "        'unique_tokens': len(set(tokens)),\n",
    "        'lexical_diversity': len(set(tokens)) / len(tokens) if tokens else 0,\n",
    "        'total_entities': len(entities),\n",
    "        'avg_sentence_length': len(tokens) / max(program.count('.'), 1)\n",
    "    })\n",
    "\n",
    "text_stats_df = pd.DataFrame(text_stats)\n",
    "comparative_df = comparative_df.merge(text_stats_df, on='partido')\n",
    "\n",
    "print(\"üìä An√°lisis comparativo integral:\")\n",
    "print(comparative_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del an√°lisis comparativo\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Sentimiento vs Diversidad L√©xica\n",
    "axes[0,0].scatter(comparative_df['lexical_diversity'], comparative_df['vader_compound'], \n",
    "                 s=100, alpha=0.7, c=comparative_df['dominant_topic_id'], cmap='tab10')\n",
    "for i, party in enumerate(comparative_df['partido']):\n",
    "    axes[0,0].annotate(party, \n",
    "                      (comparative_df['lexical_diversity'].iloc[i], \n",
    "                       comparative_df['vader_compound'].iloc[i]),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0,0].set_xlabel('Diversidad L√©xica')\n",
    "axes[0,0].set_ylabel('Sentimiento (VADER)')\n",
    "axes[0,0].set_title('Sentimiento vs Diversidad L√©xica')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Longitud vs Entidades\n",
    "axes[0,1].scatter(comparative_df['total_tokens'], comparative_df['total_entities'],\n",
    "                 s=100, alpha=0.7, c=comparative_df['dominant_topic_id'], cmap='tab10')\n",
    "for i, party in enumerate(comparative_df['partido']):\n",
    "    axes[0,1].annotate(party, \n",
    "                      (comparative_df['total_tokens'].iloc[i], \n",
    "                       comparative_df['total_entities'].iloc[i]),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "axes[0,1].set_xlabel('Total de Tokens')\n",
    "axes[0,1].set_ylabel('Total de Entidades')\n",
    "axes[0,1].set_title('Longitud vs Entidades Nombradas')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Distribuci√≥n de sentimientos por tema\n",
    "for topic_id in comparative_df['dominant_topic_id'].unique():\n",
    "    topic_data = comparative_df[comparative_df['dominant_topic_id'] == topic_id]\n",
    "    axes[0,2].scatter(topic_data['textblob_polarity'], topic_data['textblob_subjectivity'],\n",
    "                     label=f'Tema {topic_id}', s=100, alpha=0.7)\n",
    "axes[0,2].set_xlabel('Polaridad (TextBlob)')\n",
    "axes[0,2].set_ylabel('Subjetividad (TextBlob)')\n",
    "axes[0,2].set_title('Sentimientos por Tema Dominante')\n",
    "axes[0,2].legend()\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Radar chart de caracter√≠sticas por partido (seleccionar algunos partidos)\n",
    "selected_parties = ['MAS', 'MORENA', 'ADN', 'UNIDAD']\n",
    "features = ['vader_compound', 'textblob_polarity', 'lexical_diversity', 'dominant_topic_prob']\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(features), endpoint=False)\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "\n",
    "axes[1,0].set_theta_offset(np.pi / 2)\n",
    "axes[1,0].set_theta_direction(-1)\n",
    "axes[1,0] = plt.subplot(2, 3, 4, projection='polar')\n",
    "\n",
    "for party in selected_parties:\n",
    "    if party in comparative_df['partido'].values:\n",
    "        party_data = comparative_df[comparative_df['partido'] == party]\n",
    "        values = []\n",
    "        for feature in features:\n",
    "            # Normalizar valores entre 0 y 1\n",
    "            val = party_data[feature].iloc[0]\n",
    "            if feature in ['vader_compound', 'textblob_polarity']:\n",
    "                val = (val + 1) / 2  # Convertir de [-1,1] a [0,1]\n",
    "            values.append(val)\n",
    "        values += [values[0]]  # Cerrar el pol√≠gono\n",
    "        \n",
    "        axes[1,0].plot(angles, values, 'o-', linewidth=2, label=party)\n",
    "        axes[1,0].fill(angles, values, alpha=0.25)\n",
    "\n",
    "axes[1,0].set_xticks(angles[:-1])\n",
    "axes[1,0].set_xticklabels(features)\n",
    "axes[1,0].set_title('Perfil de Caracter√≠sticas\\n(Partidos Seleccionados)')\n",
    "axes[1,0].legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "\n",
    "# 5. Heatmap de correlaciones\n",
    "corr_features = ['vader_compound', 'textblob_polarity', 'textblob_subjectivity', \n",
    "                'lexical_diversity', 'total_tokens', 'total_entities', 'dominant_topic_prob']\n",
    "corr_matrix = comparative_df[corr_features].corr()\n",
    "\n",
    "im = axes[1,1].imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "axes[1,1].set_xticks(range(len(corr_features)))\n",
    "axes[1,1].set_yticks(range(len(corr_features)))\n",
    "axes[1,1].set_xticklabels([f.replace('_', '\\n') for f in corr_features], rotation=45, ha='right')\n",
    "axes[1,1].set_yticklabels([f.replace('_', '\\n') for f in corr_features])\n",
    "axes[1,1].set_title('Correlaciones entre Variables')\n",
    "\n",
    "# Agregar valores de correlaci√≥n\n",
    "for i in range(len(corr_features)):\n",
    "    for j in range(len(corr_features)):\n",
    "        text = axes[1,1].text(j, i, f'{corr_matrix.iloc[i, j]:.2f}',\n",
    "                             ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "plt.colorbar(im, ax=axes[1,1], shrink=0.8)\n",
    "\n",
    "# 6. Ranking general\n",
    "ranking_features = ['vader_compound', 'lexical_diversity', 'total_entities', 'dominant_topic_prob']\n",
    "ranking_df = comparative_df.copy()\n",
    "\n",
    "# Normalizar features para ranking\n",
    "for feature in ranking_features:\n",
    "    ranking_df[f'{feature}_rank'] = ranking_df[feature].rank(ascending=False)\n",
    "\n",
    "ranking_df['overall_rank'] = ranking_df[[f'{f}_rank' for f in ranking_features]].mean(axis=1)\n",
    "ranking_df = ranking_df.sort_values('overall_rank')\n",
    "\n",
    "axes[1,2].barh(range(len(ranking_df)), ranking_df['overall_rank'], \n",
    "              color=plt.cm.RdYlGn_r(ranking_df['overall_rank']/ranking_df['overall_rank'].max()))\n",
    "axes[1,2].set_yticks(range(len(ranking_df)))\n",
    "axes[1,2].set_yticklabels(ranking_df['partido'])\n",
    "axes[1,2].set_xlabel('Ranking Promedio')\n",
    "axes[1,2].set_title('Ranking General de Partidos')\n",
    "axes[1,2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4254a6",
   "metadata": {},
   "source": [
    "## 5. Resumen y Conclusiones del An√°lisis\n",
    "\n",
    "A continuaci√≥n se presenta un resumen de los hallazgos principales del an√°lisis de los programas de gobierno 2025:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen autom√°tico de conclusiones\n",
    "print(\"üéØ RESUMEN DEL AN√ÅLISIS DE PROGRAMAS DE GOBIERNO 2025\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. An√°lisis de frecuencias\n",
    "print(\"\\nüìä AN√ÅLISIS DE FRECUENCIAS:\")\n",
    "top_5_global = global_frequencies.most_common(5)\n",
    "print(f\"‚Ä¢ Palabras m√°s frecuentes globalmente: {', '.join([word for word, _ in top_5_global])}\")\n",
    "\n",
    "longest_program = max([(party, len(preprocess_text(program))) for party, program in programs.items()], key=lambda x: x[1])\n",
    "shortest_program = min([(party, len(preprocess_text(program))) for party, program in programs.items()], key=lambda x: x[1])\n",
    "print(f\"‚Ä¢ Programa m√°s extenso: {longest_program[0]} ({longest_program[1]} tokens)\")\n",
    "print(f\"‚Ä¢ Programa m√°s conciso: {shortest_program[0]} ({shortest_program[1]} tokens)\")\n",
    "\n",
    "# 2. An√°lisis de sentimientos\n",
    "print(\"\\nüòä AN√ÅLISIS DE SENTIMIENTOS:\")\n",
    "most_positive = comparative_df.loc[comparative_df['vader_compound'].idxmax()]\n",
    "most_negative = comparative_df.loc[comparative_df['vader_compound'].idxmin()]\n",
    "print(f\"‚Ä¢ Programa m√°s positivo (VADER): {most_positive['partido']} ({most_positive['vader_compound']:.3f})\")\n",
    "print(f\"‚Ä¢ Programa m√°s negativo (VADER): {most_negative['partido']} ({most_negative['vader_compound']:.3f})\")\n",
    "\n",
    "most_subjective = comparative_df.loc[comparative_df['textblob_subjectivity'].idxmax()]\n",
    "least_subjective = comparative_df.loc[comparative_df['textblob_subjectivity'].idxmin()]\n",
    "print(f\"‚Ä¢ Programa m√°s subjetivo: {most_subjective['partido']} ({most_subjective['textblob_subjectivity']:.3f})\")\n",
    "print(f\"‚Ä¢ Programa m√°s objetivo: {least_subjective['partido']} ({least_subjective['textblob_subjectivity']:.3f})\")\n",
    "\n",
    "# 3. Topic modeling\n",
    "print(\"\\nüè∑Ô∏è TOPIC MODELING:\")\n",
    "print(f\"‚Ä¢ Se identificaron {num_topics} temas principales\")\n",
    "for i, topic in enumerate(topics):\n",
    "    topic_parties = topic_df[topic_df['dominant_topic_id'] == i]['partido'].tolist()\n",
    "    print(f\"‚Ä¢ Tema {i}: {', '.join(topic['words'][:3])} ‚Üí Partidos: {', '.join(topic_parties)}\")\n",
    "\n",
    "# 4. Caracter√≠sticas distintivas\n",
    "print(\"\\nüîç CARACTER√çSTICAS DISTINTIVAS:\")\n",
    "most_diverse = comparative_df.loc[comparative_df['lexical_diversity'].idxmax()]\n",
    "least_diverse = comparative_df.loc[comparative_df['lexical_diversity'].idxmin()]\n",
    "print(f\"‚Ä¢ Mayor diversidad l√©xica: {most_diverse['partido']} ({most_diverse['lexical_diversity']:.3f})\")\n",
    "print(f\"‚Ä¢ Menor diversidad l√©xica: {least_diverse['partido']} ({least_diverse['lexical_diversity']:.3f})\")\n",
    "\n",
    "most_entities = comparative_df.loc[comparative_df['total_entities'].idxmax()]\n",
    "print(f\"‚Ä¢ M√°s entidades nombradas: {most_entities['partido']} ({most_entities['total_entities']} entidades)\")\n",
    "\n",
    "# 5. Ranking final\n",
    "print(\"\\nüèÜ RANKING GENERAL (basado en m√∫ltiples m√©tricas):\")\n",
    "top_3_parties = ranking_df.head(3)\n",
    "for i, (_, party_data) in enumerate(top_3_parties.iterrows(), 1):\n",
    "    print(f\"{i}. {party_data['partido']} (Rank: {party_data['overall_rank']:.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ An√°lisis completado exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b63220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados en archivos\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear directorio de resultados\n",
    "results_dir = Path('/Users/alexojeda/dev/social-data-science/resultados_analisis')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Guardar resultados del an√°lisis\n",
    "results = {\n",
    "    'fecha_analisis': datetime.now().isoformat(),\n",
    "    'resumen': {\n",
    "        'total_partidos': len(programs),\n",
    "        'total_tokens': sum(len(preprocess_text(program)) for program in programs.values()),\n",
    "        'palabras_mas_frecuentes': dict(global_frequencies.most_common(20)),\n",
    "        'temas_identificados': {\n",
    "            f'tema_{i}': {\n",
    "                'palabras_principales': topic['words'][:10],\n",
    "                'partidos_asociados': topic_df[topic_df['dominant_topic_id'] == i]['partido'].tolist()\n",
    "            }\n",
    "            for i, topic in enumerate(topics)\n",
    "        }\n",
    "    },\n",
    "    'analisis_sentimientos': comparative_df[['partido', 'vader_compound', 'textblob_polarity', 'textblob_subjectivity']].to_dict('records'),\n",
    "    'topic_modeling': topic_df[['partido', 'dominant_topic_id', 'dominant_topic_prob']].to_dict('records'),\n",
    "    'estadisticas_texto': text_stats_df.to_dict('records')\n",
    "}\n",
    "\n",
    "# Guardar en JSON\n",
    "with open(results_dir / 'analisis_programas_gobierno_2025.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Guardar DataFrames en CSV\n",
    "comparative_df.to_csv(results_dir / 'analisis_comparativo.csv', index=False, encoding='utf-8')\n",
    "sentiment_df.to_csv(results_dir / 'analisis_sentimientos.csv', index=False, encoding='utf-8')\n",
    "topic_df.to_csv(results_dir / 'topic_modeling.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"üíæ Resultados guardados en: {results_dir}\")\n",
    "print(\"üìÅ Archivos generados:\")\n",
    "print(\"  ‚Ä¢ analisis_programas_gobierno_2025.json (resumen completo)\")\n",
    "print(\"  ‚Ä¢ analisis_comparativo.csv (datos comparativos)\")\n",
    "print(\"  ‚Ä¢ analisis_sentimientos.csv (resultados de sentimientos)\")\n",
    "print(\"  ‚Ä¢ topic_modeling.csv (resultados de temas)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
